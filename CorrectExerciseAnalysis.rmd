---
title: "CorrectExerciseAnalysis"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(RCurl)
library(caret)
library(doParallel)
set.seed(9000) #His power level, it's over 9000!!
train_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
train<- read.csv(text=getURL(train_url), na.strings=c("", "NA"), row.names = 1)
test<- read.csv(text=getURL(test_url), na.strings=c("", "NA"), row.names = 1)
train<-train[,-(1:7)]
test<-test[,-(1:7)]
goodcol<-sapply(train, function (x) {(sum(is.na(x))/dim(train)[1])<0.9})
train<-train[,goodcol]
test<-test[,goodcol]
subsample<-createDataPartition(y=train$classe,p=0.75,list=FALSE)
subTrain<-train[subsample,]
subVal<-train[-subsample,]
mod1<-train(classe~.,data=subTrain,method="gbm")
mod2<-train(classe~.,data=subTrain,method="rf")
predMod1<-predict(mod1,subVal)
predMod2<-predict(mod2,subVal)
```

##Introduction
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, I use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to predict whether they performed the exercise correctly. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).  

The data includes a 'classe' variable which has 5 levels (A->E), where A is the correct exercise and B->E are incorrect variants of the exercise. I aim to create a model for predicting the classe variable from the data and test the models accuracy.
  
##Exploratory Data Analysis
Here I load the data set into a training and testset and look at the first 6 rows.  
  
```{r, eval=FALSE}
train_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
train<- read.csv(text=getURL(train_url), na.strings=c("", "NA"), row.names = 1)
test<- read.csv(text=getURL(test_url), na.strings=c("", "NA"), row.names = 1)
head(train)
```
  
##Data Tidying
The first 6 rows showed a lot of variables that will not have any correlation to the classe variable I'm going to build a model for, namely id, user_name, raw_timestamp_part_1, raw_timestamp_part_,2 cvtd_timestamp, new_window, and  num_window (columns 1 to 7). I therefore delete these columns.  
  
```{r, eval=FALSE}
train<-train[,-(1:7)]
test<-test[,-(1:7)]
```
  
A lot of the columns appeared to show a lot of NAs. I first work out the proportion of NAs for each column, it turns out that a large number of columns are ~98% NAs. Consequently I remove these columns from the training and test sets.  
  
```{r, eval=FALSE}
sapply(train, function (x) {sum(is.na(x))/dim(train)[1]})
goodcol<-sapply(train, function (x) {(sum(is.na(x))/dim(train)[1])<0.9})
train<-train[,goodcol]
test<-test[,goodcol]
```
  
I now check to see whether any of the remaining columns have near zero values, which would affect model selection and creation. None of them do.  
  
```{r}
NZVs <- nearZeroVar(train, saveMetrics = TRUE)
length(which(NZVs$nzv==TRUE))
```
  
##Model Creation and Selection
  
I first use cross validation and subset the training subset into a training and validation set so that I can work out the out of sample error on any models I train.I train two different models, one using generalised boosted regression and the other using random forests, and test their accuracy on the validation set.  
  
```{r, eval=FALSE}
subsample<-createDataPartition(y=train$classe,p=0.75,list=FALSE)
subTrain<-train[subsample,]
subVal<-train[-subsample,]
mod1<-train(classe~.,data=subTrain,method="gbm")
mod2<-train(classe~.,data=subTrain,method="rf")
predMod1<-predict(mod1,subVal)
predMod2<-predict(mod2,subVal)
```
```{r}
confusionMatrix(predMod1,subVal$classe)
confusionMatrix(predMod2,subVal$classe)
```

Based on the results the model based on random forests is the most accurate at 99.23% and an out of sample error rate of 0.77%. I will use the random forest model since it is the most accurate of the two and it works well with a large number of variables, especially when the interactions between variables are unknown. To understand the model better I plot the importance of the variables it uses.

```{r}
plot(varImp(mod2), main = "Plot of variables by importance")
```

##Test Set Prediction
```{r}
predTest<-predict(mod2,test)
predTest
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(as.character(predTest))
```
